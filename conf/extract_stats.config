# config file for the extraction of the wiki stats

# MAIN PARAMETERS 
# ie, you have to have a look at them 

# abbreviation for the language. used a bit of everywhere
language=en

# path to the dump file of wikipedia (relative to the execution point)
dump_file=data/enwiki-extract

# path to the file containing the stopwords (relative to the execution point)
stop_words=data/stopwords.en.list

# the name of the lucene analyzer
lucene_analyzer=en.EnglishAnalyzer

# path to tmp folder, to put the tmp files generated during computation
tmp_folder=data/tmp/

# path to output
output_folder=data/output/


#OTHER PARAMETERS
# maximum length (in chars) of a surface form.
MAX_LENGTH_SF=80
# Minimum length of a surface form (in char)
MIN_LENGTH_SF=2
# Max nb of token per surface form. Determined by the number of whitespace
MAX_NB_TOKEN_SF=4
# minimum number of time a couple (resource, surface form) has to appear 
# in wikipedia to be taken into consideration
MIN_OCCURENCE_COUPLE=3
# Minimum number of paragraphs containing a couple (resource, token) to
# consider the token associated with that resource
MIN_NB_CONTEXTS=2


